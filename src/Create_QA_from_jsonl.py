import json
import os
import random
import asyncio
import argparse
from collections import defaultdict
from typing import List, Set, Tuple
from pydantic import BaseModel
import openai # `WebSearch.py` ã«ã¯ç›´æ¥ `openai` ã®importã¯ãªã„ãŒã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹æƒ³å®š
from dotenv import load_dotenv

load_dotenv("/app/.env", override=True) # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€

# ãƒšãƒ«ã‚½ãƒŠå€™è£œ
PERSONAS = ["ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢", "ãƒãƒ¼ã‚±ã‚¿ãƒ¼", "å­¦ç”Ÿ", "çµŒå–¶è€…", "ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼", "å–¶æ¥­æ‹…å½“è€…", "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ", "æ³•å‹™æ‹…å½“è€…"]
# 1 URL ã‚ãŸã‚Šã®æœ€å¤§ QA æ•°
MAX_QA_PER_URL = 50
# 1å›ã®LLMå‘¼ã³å‡ºã—ã§ç”Ÿæˆã‚’è©¦ã¿ã‚‹QAæ•°
QA_PER_REQUEST = 3 # WebSearch.py ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡ç¤ºã«åˆã‚ã›ã¦èª¿æ•´

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (WebSearch.py ã® QAPair ã‚’å‚è€ƒ)
class QAPair(BaseModel):
    question: str
    answer: str
    source_url: str
    persona: str # ã©ã®ãƒšãƒ«ã‚½ãƒŠã®è³ªå•ã‹

async def generate_qa_for_text(
    url: str,
    text_content: str,
    existing_qa_for_url: List[str],
    model_name: str,
    persona: str
) -> List[QAPair]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒšãƒ«ã‚½ãƒŠã«åŸºã¥ã„ã¦QAãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    WebSearch.py ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘¼ã³å‡ºã—éƒ¨åˆ†ã‚’æ¨¡å€£ã€‚
    """
    # WebSearch.py ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡ç¤ºã‚’å‚è€ƒã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
    existing_qa_instructions_segment = "ç¾åœ¨ã€ã“ã®URLã«é–¢ã™ã‚‹æ—¢å­˜ã®Q&Aã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    if existing_qa_for_url:
        existing_qa_str = "\n".join(existing_qa_for_url)
        existing_qa_instructions_segment = (
            f"ä»¥ä¸‹ã®Q&Aãƒšã‚¢ã¯ã€ã“ã®URL ({url}) ã«é–¢ã—ã¦æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚\n"
            f"ã“ã‚Œã‚‰ã¨ã¯ç•°ãªã‚‹ã€æ–°ã—ã„æƒ…å ±ã‚„è¦–ç‚¹ã‹ã‚‰ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
            f"---æ—¢å­˜ã®Q&Aã“ã“ã‹ã‚‰---\n"
            f"{existing_qa_str}\n"
            f"---æ—¢å­˜ã®Q&Aã“ã“ã¾ã§---"
        )

    system_prompt = (
        "You are a knowledge extraction assistant.\n"
        f"1. Your primary task is to analyze the provided text content from the URL: {url}.\n"
        f"2. The user is a '{persona}'. Generate questions and answers from their perspective.\n"
        f"3. {existing_qa_instructions_segment}\n"
        f"4. From this text content, extract up to {QA_PER_REQUEST} new question-answer pairs that would be genuinely helpful. Each pair must include the source URL, which MUST be exactly '{url}', and the persona '{persona}'.\n"
        "5. Avoid duplicate or trivial questions, including those listed in the existing Q&A section if provided.\n"
        "6. The extracted question and answer MUST be in Japanese.\n"
        "Return the result as a JSON list of QAPair objects. Each QAPair object should have 'question', 'answer', 'source_url', and 'persona' fields."
    )
    user_prompt = f"URL: {url}\nPersona: {persona}\nText Content:\n{text_content}"

    try:
        # OpenAI APIå‘¼ã³å‡ºã— (WebSearch.pyã§ã¯Agentã‚¯ãƒ©ã‚¹çµŒç”±ã ãŒã€ã“ã“ã§ã¯ç›´æ¥å‘¼ã³å‡ºã™ä¾‹)
        # å®Ÿéš›ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®éåŒæœŸå‘¼ã³å‡ºã—ã«åˆã‚ã›ã‚‹ã®ãŒç†æƒ³
        response = await openai.ChatCompletion.acreate( # éåŒæœŸå‘¼ã³å‡ºã—
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"} # JSONãƒ¢ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
        )
        content = response.choices[0].message.content
        # JSONæ–‡å­—åˆ—ãŒãƒªã‚¹ãƒˆå½¢å¼ã§è¿”ã£ã¦ãã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã‚‹ãŒã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã«ã‚ˆã£ã¦ã¯èª¿æ•´ãŒå¿…è¦
        # ä¾‹: {"qa_pairs": [...]} ã®ã‚ˆã†ãªå½¢å¼ã§è¿”ã£ã¦ãã‚‹å ´åˆ
        if content:
            try:
                # ãƒ¢ãƒ‡ãƒ«ãŒç›´æ¥ãƒªã‚¹ãƒˆã‚’è¿”ã•ãªã„å ´åˆã€ã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¦æŠ½å‡º
                # ã“ã“ã§ã¯ä»®ã« 'qa_pairs' ã¨ã„ã†ã‚­ãƒ¼ã§ãƒªã‚¹ãƒˆãŒè¿”ã£ã¦ãã‚‹ã¨æƒ³å®š
                data = json.loads(content)
                qa_list_data = data.get("qa_pairs", []) if isinstance(data, dict) else data

                # QAPairã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                generated_qa_pairs = []
                for item in qa_list_data:
                    if isinstance(item, dict) and "question" in item and "answer" in item:
                         # ãƒ¢ãƒ‡ãƒ«ãŒ source_url ã¨ persona ã‚’å«ã‚ã¦ãã‚Œãªã„å ´åˆã€ã“ã“ã§ä»˜ä¸
                        generated_qa_pairs.append(QAPair(
                            question=item["question"],
                            answer=item["answer"],
                            source_url=url, # LLMãŒæ­£ã—ãä»˜ä¸ã—ãªã„å ´åˆãŒã‚ã‚‹ã®ã§å¼·åˆ¶
                            persona=persona # LLMãŒæ­£ã—ãä»˜ä¸ã—ãªã„å ´åˆãŒã‚ã‚‹ã®ã§å¼·åˆ¶
                        ))
                return generated_qa_pairs
            except json.JSONDecodeError as e:
                print(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}, Content: {content}")
                return []
            except Exception as e:
                print(f"QAãƒšã‚¢ã®ãƒ‘ãƒ¼ã‚¹ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
                return []
        return []
    except Exception as e:
        print(f"OpenAI APIå‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return []


async def process_url_content(
    url: str,
    full_text_content: str,
    outfile: str,
    model_name: str,
    max_qa_per_url: int
):
    """
    å˜ä¸€URLã®å…¨æ–‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰QAã‚’ç”Ÿæˆã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ã™ã‚‹ã€‚
    WebSearch.py ã® collect_qa ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‚è€ƒã«ã™ã‚‹ã€‚
    """
    print(f"\n--- URLã®å‡¦ç†é–‹å§‹: {url} ---")
    # æ—¢å­˜ã®QAã‚’èª­ã¿è¾¼ã‚€ (ã“ã®URLã«ç‰¹åŒ–ã—ãŸã‚‚ã® + å…¨ä½“ã§é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã®ã‚‚ã®)
    existing_qa_for_this_url_display: List[str] = [] # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨
    all_existing_qa_tuples: Set[Tuple[str, str]] = set() # å…¨ä½“ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ (è³ªå•, å›ç­”)

    if os.path.exists(outfile):
        try:
            with open(outfile, "r", encoding="utf-8") as reader:
                for line in reader:
                    try:
                        qa_obj_dict = json.loads(line)
                        q = qa_obj_dict.get("question")
                        a = qa_obj_dict.get("answer")
                        if q and a:
                            all_existing_qa_tuples.add((q, a))
                            if qa_obj_dict.get("source_url") == url:
                                existing_qa_for_this_url_display.append(f"- Q: {q}\n  A: {a}")
                    except json.JSONDecodeError:
                        print(f"è­¦å‘Š: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{outfile}' ã®è¡Œã®JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {line.strip()}")
        except Exception as e:
            print(f"è­¦å‘Š: æ—¢å­˜ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{outfile}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    generated_qa_for_this_url_count = 0
    # ã“ã®URLã«å¯¾ã—ã¦æ—¢ã«ç”Ÿæˆã•ã‚ŒãŸQAã®æ•°ã‚’åˆæœŸåŒ– (ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã æ•°ã§ã¯ãªã„)
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¾ã‚ŒãŸQAã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ãŸã‚ã€ãƒ«ãƒ¼ãƒ—å†…ã§ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã™ã‚‹

    # è©¦è¡Œå›æ•°ã‚„ãƒ«ãƒ¼ãƒ—æ¡ä»¶ã¯ WebSearch.py ã® collect_qa ã‚’å‚è€ƒã«ã™ã‚‹
    # ã“ã“ã§ã¯ã€MAX_QA_PER_URLã«é”ã™ã‚‹ã‹ã€æ–°ã—ã„QAãŒç”Ÿæˆã•ã‚Œãªããªã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™
    # è¤‡æ•°ã®ãƒšãƒ«ã‚½ãƒŠã‚’è©¦ã™ãŸã‚ã€è©¦è¡Œå›æ•°ã«ã¯ä½™è£•ã‚’æŒãŸã›ã‚‹
    max_attempts_per_persona_cycle = (max_qa_per_url // QA_PER_REQUEST) + len(PERSONAS) + 5 # ååˆ†ãªè©¦è¡Œå›æ•°

    for attempt in range(max_attempts_per_persona_cycle):
        if generated_qa_for_this_url_count >= max_qa_per_url:
            print(f"â„¹ï¸ URL '{url}' ã®QAç”Ÿæˆæ•°ãŒä¸Šé™ ({max_qa_per_url}) ã«é”ã—ã¾ã—ãŸã€‚")
            break

        current_persona = random.choice(PERSONAS)
        print(f"è©¦è¡Œ {attempt + 1}/{max_attempts_per_persona_cycle}, ãƒšãƒ«ã‚½ãƒŠ: {current_persona}, URL: {url}")

        # LLMã‹ã‚‰QAãƒšã‚¢ã‚’å–å¾—
        # æ—¢å­˜ã®QAæƒ…å ±ã‚’æ¸¡ã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹ã‚ˆã†ã«æŒ‡ç¤º
        newly_generated_pairs: List[QAPair] = await generate_qa_for_text(
            url,
            full_text_content,
            existing_qa_for_this_url_display, # ã“ã®URLã«é–¢ã™ã‚‹æ—¢å­˜QAã‚’æ¸¡ã™
            model_name,
            current_persona
        )

        if not newly_generated_pairs:
            print(f"â„¹ï¸ ãƒšãƒ«ã‚½ãƒŠ '{current_persona}' ã§ã¯æ–°ã—ã„QAã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            # é€£ç¶šã§ç”Ÿæˆå¤±æ•—ã™ã‚‹å ´åˆã€ãƒšãƒ«ã‚½ãƒŠã‚’å¤‰ãˆã¦ã‚‚åŠ¹æœãŒè–„ã„å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€
            # ä½•å›ã‹å¤±æ•—ã—ãŸã‚‰ãã®URLã®å‡¦ç†ã‚’æ‰“ã¡åˆ‡ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚‚æ¤œè¨ã§ãã‚‹
            if attempt > len(PERSONAS) * 2 and not newly_generated_pairs : # ç°¡æ˜“çš„ãªæ‰“ã¡åˆ‡ã‚Šæ¡ä»¶
                 print(f"â„¹ï¸ URL '{url}' ã§è¤‡æ•°å›æ–°ã—ã„QAã‚’ç”Ÿæˆã§ããªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                 break
            await asyncio.sleep(1) # çŸ­ã„å¾…æ©Ÿ
            continue

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
        added_in_this_attempt_count = 0
        with open(outfile, "a", encoding="utf-8") as writer:
            for qa_pair in newly_generated_pairs:
                if generated_qa_for_this_url_count >= max_qa_per_url:
                    break # ã“ã®URLã®QAä¸Šé™ã«é”ã—ãŸã‚‰ã“ã®è©¦è¡Œã®æ®‹ã‚Šã‚‚ã‚¹ã‚­ãƒƒãƒ—

                current_qa_tuple = (qa_pair.question, qa_pair.answer)
                # å…¨ä½“ã®æ—¢å­˜QAã‚»ãƒƒãƒˆã¨ã€ã“ã®URLã§ä»Šå›è¿½åŠ ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã‚‚ã®ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if current_qa_tuple not in all_existing_qa_tuples:
                    writer.write(json.dumps(qa_pair.model_dump(), ensure_ascii=False) + "\n")
                    all_existing_qa_tuples.add(current_qa_tuple) # å…¨ä½“ã‚»ãƒƒãƒˆã«è¿½åŠ 
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã®æ—¢å­˜QAãƒªã‚¹ãƒˆã«ã‚‚è¿½åŠ  (æ¬¡ã®LLMå‘¼ã³å‡ºã—ã®ãŸã‚)
                    existing_qa_for_this_url_display.append(f"- Q: {qa_pair.question}\n  A: {qa_pair.answer}")
                    generated_qa_for_this_url_count += 1
                    added_in_this_attempt_count += 1
                else:
                    print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(é‡è¤‡): Q: {qa_pair.question}")

        if added_in_this_attempt_count > 0:
            print(f"âœ¨ ã“ã®è©¦è¡Œã§ {added_in_this_attempt_count} ä»¶ã‚’æ–°ãŸã«æ›¸ãå‡ºã—ã¾ã—ãŸ (URL: {url}, Persona: {current_persona})ã€‚")
            print(f"ç´¯è¨ˆ: {generated_qa_for_this_url_count}/{max_qa_per_url} (URL: {url})")
        else:
            print(f"â„¹ï¸ ã“ã®è©¦è¡Œã§ã¯é‡è¤‡ã—ãªã„æ–°ã—ã„QAã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ (URL: {url}, Persona: {current_persona})ã€‚")

        if generated_qa_for_this_url_count >= max_qa_per_url: # å†åº¦ãƒã‚§ãƒƒã‚¯
            print(f"â„¹ï¸ URL '{url}' ã®QAç”Ÿæˆæ•°ãŒä¸Šé™ ({max_qa_per_url}) ã«é”ã—ã¾ã—ãŸã€‚")
            break
        
        await asyncio.sleep(random.uniform(1, 3)) # APIè² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ

    print(f"--- URLã®å‡¦ç†å®Œäº†: {url}, åˆè¨ˆ {generated_qa_for_this_url_count} ä»¶ã®QAã‚’ç”Ÿæˆ ---")
    return generated_qa_for_this_url_count


async def main_process(input_jsonl_file: str, outfile: str, model_name: str, max_qa_per_url: int):
    # aflac_with_body.jsonl ã‹ã‚‰ URL ã¨æœ¬æ–‡ã‚’èª­ã¿è¾¼ã‚€
    # WebSearch.py ã§ã¯å˜ä¸€URLã‚’å¼•æ•°ã«å–ã‚‹ãŒã€ã“ã¡ã‚‰ã¯JSONLå†…ã®å…¨URLã‚’å‡¦ç†
    url_to_text_map = defaultdict(str)
    if not os.path.exists(input_jsonl_file):
        print(f"ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{input_jsonl_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    with open(input_jsonl_file, "r", encoding="utf-8") as f_in:
        for line in f_in:
            try:
                data = json.loads(line)
                url = data.get("url")
                body = data.get("body", "")
                if url and body:
                    # åŒä¸€URLã®æœ¬æ–‡ã¯é€£çµã—ã¦ã„ãï¼ˆWebSearch.pyã«ã¯ã“ã®é›†ç´„ãƒ­ã‚¸ãƒƒã‚¯ã¯ãªã„ï¼‰
                    url_to_text_map[url] += body + "\n"
            except json.JSONDecodeError:
                print(f"è­¦å‘Š: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{input_jsonl_file}' ã®è¡Œã®JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—: {line.strip()}")

    if not url_to_text_map:
        print(f"ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{input_jsonl_file}' ã‹ã‚‰å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    print(f"{len(url_to_text_map)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯URLã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™ã€‚")
    total_qa_generated_all_urls = 0

    # å„URLã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å¯¾ã—ã¦QAç”Ÿæˆå‡¦ç†ã‚’å‘¼ã³å‡ºã™
    # asyncio.gather ã‚’ä½¿ã£ã¦ä¸¦åˆ—å‡¦ç†ã‚‚å¯èƒ½ã ãŒã€APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚„ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã®ç«¶åˆã‚’è€ƒæ…®ã—ã€
    # ã“ã“ã§ã¯é †æ¬¡å‡¦ç†ã¨ã™ã‚‹ã€‚ä¸¦åˆ—åŒ–ã™ã‚‹å ´åˆã¯æ›¸ãè¾¼ã¿å‡¦ç†ã®æ’ä»–åˆ¶å¾¡ãŒå¿…è¦ã€‚
    for url, full_text in url_to_text_map.items():
        if not full_text.strip():
            print(f"â„¹ï¸ URL '{url}' ã®æœ¬æ–‡ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
        count = await process_url_content(url, full_text, outfile, model_name, max_qa_per_url)
        total_qa_generated_all_urls += count
        # URLé–“ã®å‡¦ç†ã«ã‚‚å°‘ã—é–“éš”ã‚’ç©ºã‘ã‚‹
        await asyncio.sleep(random.uniform(2, 5))


    print(f"\n--- å…¨å‡¦ç†å®Œäº† ---")
    print(f"ğŸ‰ åˆè¨ˆ {total_qa_generated_all_urls} ä»¶ã®æ–°ã—ã„Q&Aã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã«æ›¸ãå‡ºã—ã¾ã—ãŸ â†’ {outfile}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æŒ‡å®šã•ã‚ŒãŸJSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿ã€URLã”ã¨ã«QAãƒšã‚¢ã‚’åé›†ã—ã¾ã™ã€‚")
    parser.add_argument(
        "--input_file",
        type=str,
        default="aflac_with_body.jsonl",
        help="å…¥åŠ›JSONLãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: aflac_with_body.jsonl)"
    )
    parser.add_argument(
        "--outfile",
        type=str,
        default="generated_qa_from_jsonl.jsonl",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: output_qa.jsonl)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-3.5-turbo", # ã‚ˆã‚Šé«˜é€Ÿãƒ»å®‰ä¾¡ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
        help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å (ä¾‹: gpt-4o, gpt-3.5-turbo)"
    )
    parser.add_argument(
        "--max_qa_per_url",
        type=int,
        default=MAX_QA_PER_URL,
        help=f"1ã¤ã®URLã‹ã‚‰ç”Ÿæˆã™ã‚‹æœ€å¤§ã®QAæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {MAX_QA_PER_URL})"
    )
    # WebSearch.py ã«ã‚ã£ãŸ max_attempts ã¯ã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯
    # max_qa_per_url ã¨ QA_PER_REQUEST, PERSONAS ã®æ•°ã‹ã‚‰å‹•çš„ã«è¨ˆç®—ã•ã‚Œã‚‹ãŸã‚ã€ç›´æ¥ã®å¼•æ•°ã«ã¯ã—ãªã„ã€‚

    args = parser.parse_args()

    # OPENAI_API_KEY ã®ãƒã‚§ãƒƒã‚¯
    if not os.getenv("OPENAI_API_KEY"):
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å…ˆé ­è¿‘ãã«ã‚ã‚‹ load_dotenv('/app/.env', override=True) ã‚’ç¢ºèªã™ã‚‹ã‹ã€ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    else:
        asyncio.run(main_process(args.input_file, args.outfile, args.model, args.max_qa_per_url))
