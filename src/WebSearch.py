# %%
from agents import Agent, Runner, WebSearchTool
from pydantic import BaseModel
from typing import List
import jsonlines, asyncio, os
import argparse # argparse ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from dotenv import load_dotenv
from urllib.parse import urlparse # urllib.parseã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

load_dotenv("/app/.env", override=True)        # OPENAI_API_KEY ã‚’èª­ã¿è¾¼ã‚€

# %%
# 1ï¸âƒ£  å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
class QAPair(BaseModel):
    question: str
    answer: str
    source_url: str # å‚ç…§å…ƒURLã‚’æ ¼ç´ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 

def extract_search_domain(domain_str: str) -> str | None:
    """
    ãƒ‰ãƒ¡ã‚¤ãƒ³æ–‡å­—åˆ—ã‹ã‚‰æ¤œç´¢ç”¨ã®ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ›ã‚¹ãƒˆåã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    ä¾‹: "https://example.com/path" -> "example.com"
    """
    if not domain_str:
        return None
    
    temp_domain_str = domain_str
    if '://' not in temp_domain_str:
        temp_domain_str = 'http://' + temp_domain_str # urlparseãŒãƒ›ã‚¹ãƒˆåã‚’æ­£ã—ãè§£é‡ˆã§ãã‚‹ã‚ˆã†ã«ã‚¹ã‚­ãƒ¼ãƒ ã‚’è¿½åŠ 
    
    parsed = urlparse(temp_domain_str)
    return parsed.hostname

# 3ï¸âƒ£  Runner ã§å®Ÿè¡Œã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
async def collect_qa(target_url: str, outfile: str, model_name: str, max_attempts: int = 5): # domain ã‚’ target_url ã«å¤‰æ›´ã€max_attempts ã‚’è¿½åŠ 
    # search_domain = extract_search_domain(target_url) # å˜ä¸€URLæŒ‡å®šã®ãŸã‚ã€ãƒ‰ãƒ¡ã‚¤ãƒ³æŠ½å‡ºã¯æŒ‡ç¤ºã‚„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ç›´æ¥ä½¿ã‚ãªã„
    if not target_url: # target_url ãŒç©ºã‹ãƒã‚§ãƒƒã‚¯
        print(f"ã‚¨ãƒ©ãƒ¼: å…¥åŠ› URL ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
        return

    total_newly_added_in_session = 0
    attempt_count = 0

    while attempt_count < max_attempts:
        attempt_count += 1
        print(f"\\n--- è©¦è¡Œå›æ•°: {attempt_count}/{max_attempts} ---")

        existing_qa_set = set()
        existing_qa_for_target_url_display = [] # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®æŒ‡ç¤ºã«å«ã‚ã‚‹ãŸã‚ã®æ—¢å­˜Q&Aãƒªã‚¹ãƒˆ
        if os.path.exists(outfile):
            try:
                with jsonlines.open(outfile, "r") as reader:
                    for qa_obj_dict in reader:
                        # question ã¨ answer ã®ã‚¿ãƒ—ãƒ«ã‚’ã‚»ãƒƒãƒˆã«è¿½åŠ ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯ã«åˆ©ç”¨
                        existing_qa_set.add((qa_obj_dict.get("question"), qa_obj_dict.get("answer")))
                        # ç¾åœ¨ã®target_urlã«é–¢é€£ã™ã‚‹æ—¢å­˜Q&Aã‚’åé›†
                        if qa_obj_dict.get("source_url") == target_url:
                            q = qa_obj_dict.get("question")
                            a = qa_obj_dict.get("answer")
                            if q and a: # è³ªå•ã¨å›ç­”ãŒä¸¡æ–¹å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿
                                existing_qa_for_target_url_display.append(f"- Q: {q}\\\\n  A: {a}")
            except Exception as e:
                print(f"è­¦å‘Š: æ—¢å­˜ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{outfile}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        existing_qa_instructions_segment = "ç¾åœ¨ã€ã“ã®URLã«é–¢ã™ã‚‹æ—¢å­˜ã®Q&Aã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        if existing_qa_for_target_url_display:
            existing_qa_str = "\\\\n".join(existing_qa_for_target_url_display)
            existing_qa_instructions_segment = (
                f"ä»¥ä¸‹ã®Q&Aãƒšã‚¢ã¯ã€ã“ã®URL ({target_url}) ã«é–¢ã—ã¦æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚\\\\\\\\n"
                f"ã“ã‚Œã‚‰ã¨ã¯ç•°ãªã‚‹ã€æ–°ã—ã„æƒ…å ±ã‚„è¦–ç‚¹ã‹ã‚‰ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\\\\\\\\n"
                f"---æ—¢å­˜ã®Q&Aã“ã“ã‹ã‚‰---\\\\\\\\n"
                f"{existing_qa_str}\\\\\\\\n"
                f"---æ—¢å­˜ã®Q&Aã“ã“ã¾ã§---"
            )

        qa_agent = Agent(
            name        = "Web QA Collector",
            instructions=(
                "You are a knowledge extraction assistant.\\\\\\\\n"
                f"1. Your primary task is to analyze the content of a single, specific web page: {target_url}. Use the WebSearchTool for this purpose. Do NOT navigate away from this URL. Do NOT follow any links on the page. All information must come strictly from the content of {target_url}.\\\\\\\\n"
                f"2. Read and understand the content of the page at {target_url}.\\\\\\\\n"
                f"3. {existing_qa_instructions_segment}\\\\\\\\n" # æ—¢å­˜Q&Aæƒ…å ±ã‚’æŒ‡ç¤ºã«è¿½åŠ 
                f"4. From this single page ({target_url}), extract up to 3 new question-answer pairs that would be genuinely helpful for an FAQ, considering the existing Q&A above. Each pair must include the source URL, and this source URL MUST be exactly '{target_url}'.\\\\\\\\n"
                "5. Avoid duplicate / trivial questions, including those listed in the existing Q&A section if provided.\\\\\\\\n"
                "6. The extracted question and answer MUST be in Japanese. If the source content is in another language, translate them to Japanese.\\\\\\\\n"
                "Return the result as List[QAPair]."
            ),
            tools       = [WebSearchTool(search_context_size="high")],
            output_type = List[QAPair],      # â† ã“ã‚ŒãŒè¿”ã‚‹ã¾ã§è‡ªå‹•çš„ã«ãƒ«ãƒ¼ãƒ—
            model       = model_name
        )
        # site æ¤œç´¢ã§ã¯ãªãã€ç›´æ¥ target_url ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å…¥åŠ›ã¨ã™ã‚‹
        result = await Runner.run(qa_agent, input=target_url)

        current_run_added_count = 0
        filtered_output_this_attempt = []
        processed_in_current_run_this_attempt = set() # ä»Šå›ã®å®Ÿè¡Œã®è©¦è¡Œã§å‡¦ç†æ¸ˆã¿ã®Q&Aã‚’ä¿æŒã™ã‚‹ã‚»ãƒƒãƒˆ


        if result.final_output:
            for qa in result.final_output:
                if qa and qa.source_url: # qaã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨source_urlãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
                    # qa_source_hostname = extract_search_domain(qa.source_url) # ãƒ‰ãƒ¡ã‚¤ãƒ³å˜ä½ã®ãƒã‚§ãƒƒã‚¯ã‹ã‚‰URLå®Œå…¨ä¸€è‡´ã«å¤‰æ›´
                    if qa.source_url == target_url: # å‚ç…§å…ƒURLãŒæŒ‡å®šã•ã‚ŒãŸURLã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
                        # ç¾åœ¨ã®å®Ÿè¡Œã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨ã€æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        current_qa_tuple = (qa.question, qa.answer)
                        if current_qa_tuple not in existing_qa_set and current_qa_tuple not in processed_in_current_run_this_attempt:
                            filtered_output_this_attempt.append(qa)
                            processed_in_current_run_this_attempt.add(current_qa_tuple) # ä»Šå›å‡¦ç†ã—ãŸQ&Aã¨ã—ã¦è¿½åŠ 
                        else:
                            print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(é‡è¤‡): {qa.question}")
                    else:
                        print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(URLä¸ä¸€è‡´): {qa.source_url} (æœŸå¾…: {target_url})")
                elif qa:
                     print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(source_urlãªã—): {qa}")
                # else: qaãŒNoneã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
        
        if filtered_output_this_attempt:
            with jsonlines.open(outfile, "a") as writer: # "w" ã‹ã‚‰ "a" (è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰) ã«å¤‰æ›´
                for qa_pair in filtered_output_this_attempt: # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
                    writer.write(qa_pair.model_dump())
            current_run_added_count = len(filtered_output_this_attempt)
            total_newly_added_in_session += current_run_added_count
            print(f"âœ¨ ã“ã®è©¦è¡Œã§ {current_run_added_count} ä»¶ã‚’æ–°ãŸã«æ›¸ãå‡ºã—ã¾ã—ãŸã€‚")
        else:
            print("â„¹ï¸ ã“ã®è©¦è¡Œã§ã¯æ–°ã—ã„Q&Aã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            # æ–°ã—ã„Q&AãŒãªã‘ã‚Œã°ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†
            break
            
        # çŸ­ã„å¾…æ©Ÿæ™‚é–“ã‚’å…¥ã‚Œã‚‹ (APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã‚„ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒã˜çµæœã‚’è¿”ã—ç¶šã‘ã‚‹ã®ã‚’é¿ã‘ã‚‹ãŸã‚)
        await asyncio.sleep(5) # 5ç§’å¾…æ©Ÿ

    print(f"\\n--- å…¨è©¦è¡Œå®Œäº† ---")
    print(f"ğŸ‰ åˆè¨ˆ {total_newly_added_in_session} ä»¶ã®æ–°ã—ã„Q&Aã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã«æ›¸ãå‡ºã—ã¾ã—ãŸ â†’ {outfile}")
    
    # å…ƒã®çµ±è¨ˆæƒ…å ±è¡¨ç¤ºéƒ¨åˆ†ã¯ãƒ«ãƒ¼ãƒ—ã®å¤–ã«ç§»å‹•ã€ã¾ãŸã¯ãƒ«ãƒ¼ãƒ—å†…ã®æƒ…å ±ã‚’é›†ç´„ã—ã¦è¡¨ç¤ºã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ãŒå¿…è¦
    # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã€ãƒ«ãƒ¼ãƒ—å¾Œã®ç·æ‹¬çš„ãªè¡¨ç¤ºã®ã¿ã¨ã™ã‚‹

# %%
# 4ï¸âƒ£  å®Ÿè¡Œ
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æŒ‡å®šã•ã‚ŒãŸå˜ä¸€ã®Webãƒšãƒ¼ã‚¸ã‹ã‚‰Q&Aãƒšã‚¢ã‚’åé›†ã—ã¾ã™ã€‚") # èª¬æ˜ã‚’æ›´æ–°
    parser.add_argument(
        "--url", # domain ã‚’ url ã«å¤‰æ›´
        type=str,
        required=True, # URLã¯å¿…é ˆã¨ã™ã‚‹
        help="æ¤œç´¢å¯¾è±¡ã®å®Œå…¨ãªURL (ä¾‹: https://example.com/mypage)" # ãƒ˜ãƒ«ãƒ—ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°
    )
    parser.add_argument(
        "--outfile",
        type=str,
        default="python_docs_faq.jsonl",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: output.jsonl)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o",
        help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å (ä¾‹: gpt-4o-mini, gpt-4.1)"
    )
    parser.add_argument(
        "--max_attempts", # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æœ€å¤§è©¦è¡Œå›æ•°ã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
        type=int,
        default=5, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æœ€å¤§è©¦è¡Œå›æ•°
        help="æ–°ã—ã„Q&Aã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®æœ€å¤§è©¦è¡Œå›æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)"
    )
    args = parser.parse_args()

    asyncio.run(collect_qa(args.url, args.outfile, args.model, args.max_attempts)) # args.domain ã‚’ args.url ã«å¤‰æ›´, max_attempts ã‚’è¿½åŠ 