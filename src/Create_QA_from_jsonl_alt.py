\
import asyncio
import jsonlines
import os
import argparse
from typing import List, Set, Tuple, Dict, Any
from pydantic import BaseModel
from dotenv import load_dotenv

# agentsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒ Create_QA ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã¨ä»®å®š
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from agents import Agent, Runner # agentsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰Agentã¨Runnerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

load_dotenv("/app/.env", override=True)

# --- WebSearch.pyã‹ã‚‰æµç”¨ ---
class QAPair(BaseModel):
    question: str
    answer: str
    source_url: str # JSONLã®å„ã‚¨ãƒ³ãƒˆãƒªã®å‡ºå…¸ã‚’ç¤ºã™ãŸã‚ã«æµç”¨
    questioner_persona: str # è¿½åŠ : ã©ã®ã‚ˆã†ãªäººãŒã™ã‚‹è³ªå•ã‹
    information_category: str  # è¿½åŠ : æƒ…å ±ã®ã‚«ãƒ†ã‚´ãƒª
    related_keywords: List[str] # è¿½åŠ : é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

# --- WebSearch.pyã®generate_qa_for_urlã‚’æ”¹å¤‰ ---
async def generate_qa_for_content(
    source_identifier: str, # URLã‚„ãƒ•ã‚¡ã‚¤ãƒ«åãªã©ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡ºå…¸
    text_content: str,
    existing_qa_for_source_display: List[str],
    model_name: str,
    max_q_per_content: int
) -> List[QAPair]:
    """
    ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    if not existing_qa_for_source_display:
        existing_qa_instructions_segment = "There are currently no existing Q&A pairs for this source."
    else:
        existing_qa_str = "\\\\n".join(existing_qa_for_source_display) # existing_qa_for_source_display ã¯ "Q: ...\\nA: ..." å½¢å¼ã®æ–‡å­—åˆ—ãƒªã‚¹ãƒˆã‚’æƒ³å®š
        existing_qa_instructions_segment = (
            f"The following Q&A pairs already exist for this source ({source_identifier}):\\\\n"
            f"Please generate new Q&A pairs that are different from these, offering new information or perspectives.\\\\n"
            f"---Existing Q&A Start---\\\\n"
            f"{existing_qa_str}\\\\n"
            f"---Existing Q&A End---"
        )

    qa_agent = Agent(
        name="JSONL Content QA Extractor",
        instructions=(
            "You are a knowledge extraction assistant.\\\\n"
            f"1. Your primary task is to analyze the provided text content which was extracted from the source: {source_identifier} (likely a life insurance company's webpage).\\\\n"
            f"2. The provided text content is: \\\\\\\\n---TEXT CONTENT BEGIN---\\\\\\\\n{text_content}\\\\\\\\n---TEXT CONTENT END---\\\\\\\\n"
            f"3. {existing_qa_instructions_segment}\\\\\\\\n"
            f"4. From THIS TEXT CONTENT, extract up to {max_q_per_content} new question-answer pairs that would be genuinely helpful for an FAQ, considering the existing Q&A above. Each pair must include:\\\\n"
            "    a. The question (in Japanese).\\\\n"
            "    b. The answer (in Japanese).\\\\n"
            f"   c. The source identifier, which MUST be exactly '{source_identifier}'. (This will be the 'source_url' in the output QAPair object)\\\\n"
            "    d. A brief description of the type of person who would ask this question, considering they are likely viewing a life insurance company's website (e.g., 'å¥‘ç´„æ¤œè¨ä¸­ã®é¡§å®¢', 'æ—¢å¥‘ç´„è€…', 'ä¿é™ºé‡‘å—å–äºº', 'å°±è·æ´»å‹•ä¸­ã®å­¦ç”Ÿ', 'ä¸€èˆ¬çš„ãªæƒ…å ±åé›†è€…') - this is the 'questioner_persona' (in Japanese).\\\\n"
            "    e. An appropriate 'information_category' for this Q&A (e.g., 'å¥‘ç´„æ‰‹ç¶šã', 'ä¿éšœå†…å®¹', 'ä¿é™ºé‡‘è«‹æ±‚', 'å•†å“æ¯”è¼ƒ', 'ç¨é‡‘ãƒ»æ§é™¤', 'å¥åº·å¢—é€²ã‚µãƒ¼ãƒ“ã‚¹', 'ä¼šç¤¾æƒ…å ±') (in Japanese).\\\\n"
            "    f. A list of 'related_keywords' (3-5 keywords) relevant to this Q&A (in Japanese).\\\\n"
            "5. Avoid duplicate / trivial questions, including those listed in the existing Q&A section if provided.\\\\\\\\n"
            "6. The extracted question, answer, questioner_persona, information_category, and related_keywords MUST be in Japanese. If the source content is in another language, translate them to Japanese.\\\\\\\\n"
            "7. The answer should be self-contained and directly address the question. Avoid answers that primarily redirect the user elsewhere (e.g., 'Please refer to page X', 'Contact customer support for this'). If the provided text only allows for a redirecting answer, then do not generate a Q&A pair for that specific point.\\\\n"
            "Return the result as List[QAPair]. Ensure each QAPair object includes 'question', 'answer', 'source_url', 'questioner_persona', 'information_category', and 'related_keywords'."
        ),
        # WebSearchToolã¯ä¸è¦
        output_type=List[QAPair],
        model=model_name,
    )

    # Agentã¸ã®å…¥åŠ›ã¯ã€åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãã®ã‚‚ã®ã§ã¯ãªãã€
    # Agentã®instructionså†…ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç›´æ¥æ‰±ã£ã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã¯source_identifierã‚’æ¸¡ã—ã¾ã™ã€‚
    result = await Runner.run(qa_agent, input=f"Generate Q&A for content from {source_identifier}")

    if result.final_output:
        # WebSearch.pyã¨åŒæ§˜ã«ã€source_urlãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å‡ºå…¸æƒ…å ±ã‚’å¼·åˆ¶çš„ã«è¨­å®š
        processed_qas = []
        for qa in result.final_output:
            qa_dict = qa.model_dump()
            if qa_dict.get("source_url") != source_identifier:
                # print(f"ä¿®æ­£: agentãŒè¿”ã—ãŸsource_url '{qa_dict.get('source_url')}' ã‚’ '{source_identifier}' ã«ä¿®æ­£ã—ã¾ã™ã€‚")
                qa_dict["source_url"] = source_identifier
            processed_qas.append(QAPair(**qa_dict))
        return processed_qas
    return []

# --- WebSearch.pyã®mainå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ”¹å¤‰ ---
async def process_jsonl_and_generate_qa(
    input_jsonl_path: str,
    outfile: str,
    model_name: str,
    source_id_field: str, # JSONLå†…ã®å‡ºå…¸IDãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
    content_field: str,   # JSONLå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
    max_q_per_entry: int = 3,
    max_entries_to_process: int = -1,
    # WebSearch.pyã«ã‚ã£ãŸmax_tries_per_urlã¯ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒé™çš„ãªã®ã§å˜ç´”åŒ–ã®ãŸã‚å‰Šé™¤
):
    """
    JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã€å„ã‚¨ãƒ³ãƒˆãƒªã®æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰Q&Aã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    WebSearch.pyã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚
    """
    if not os.path.exists(input_jsonl_path):
        print(f"ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{input_jsonl_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # --- WebSearch.pyã‹ã‚‰æµç”¨: æ—¢å­˜Q&Aã®èª­ã¿è¾¼ã¿ ---
    existing_qa_set: Set[Tuple[str, str]] = set()
    if os.path.exists(outfile):
        try:
            with jsonlines.open(outfile, "r") as reader:
                for qa_obj_dict in reader:
                    existing_qa_set.add((qa_obj_dict.get("question"), qa_obj_dict.get("answer")))
            print(f"æ—¢å­˜ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{outfile}' ã‹ã‚‰ {len(existing_qa_set)} ä»¶ã®Q&Aã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"è­¦å‘Š: æ—¢å­˜ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{outfile}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    total_newly_added_in_session = 0
    processed_entry_count = 0

    # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãƒ«ãƒ¼ãƒ— (WebSearch.pyã®URLãƒªã‚¹ãƒˆãƒ«ãƒ¼ãƒ—ã®ä»£ã‚ã‚Š)
    with jsonlines.open(input_jsonl_path, "r") as reader:
        for i, entry in enumerate(reader):
            if max_entries_to_process != -1 and processed_entry_count >= max_entries_to_process:
                print(f"æŒ‡å®šã•ã‚ŒãŸæœ€å¤§å‡¦ç†ã‚¨ãƒ³ãƒˆãƒªæ•° ({max_entries_to_process}) ã«é”ã—ã¾ã—ãŸã€‚")
                break
            
            print(f"\\n--- JSONLã‚¨ãƒ³ãƒˆãƒª {i+1} ã‚’å‡¦ç†ä¸­ ---")

            source_identifier = entry.get(source_id_field)
            text_content = entry.get(content_field)

            if not source_identifier:
                print(f"è­¦å‘Š: ã‚¨ãƒ³ãƒˆãƒª {i+1} ã« '{source_id_field}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
            if not text_content:
                print(f"è­¦å‘Š: ã‚¨ãƒ³ãƒˆãƒª {i+1} ã« '{content_field}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            print(f"ã‚½ãƒ¼ã‚¹ID: {source_identifier}")
            # print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (å…ˆé ­100æ–‡å­—): {text_content[:100]}...") # ãƒ‡ãƒãƒƒã‚°ç”¨

            # --- WebSearch.pyã‹ã‚‰æµç”¨: ç¾åœ¨ã®ã‚½ãƒ¼ã‚¹ã«é–¢ã™ã‚‹æ—¢å­˜Q&Aã®åé›† ---
            existing_qa_for_current_source_display: List[str] = []
            if os.path.exists(outfile): # å†åº¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãã®ã¯éåŠ¹ç‡ã ãŒWebSearch.pyã®æ§‹é€ ã«åˆã‚ã›ã‚‹
                try:
                    with jsonlines.open(outfile, "r") as r:
                        for qa_obj_dict in r:
                            if qa_obj_dict.get("source_url") == source_identifier: # QAPairã®source_urlãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‚ç…§
                                q = qa_obj_dict.get("question")
                                a = qa_obj_dict.get("answer")
                                if q and a:
                                    existing_qa_for_current_source_display.append(f"- Q: {q}\\\\n  A: {a}")
                except Exception as e:
                    print(f"è­¦å‘Š: æ—¢å­˜ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{outfile}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ (ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†ä¸­): {e}")
            
            # --- WebSearch.pyã‹ã‚‰æµç”¨: Q&Aç”Ÿæˆå‘¼ã³å‡ºã— ---
            generated_qas = await generate_qa_for_content(
                source_identifier,
                text_content,
                existing_qa_for_current_source_display,
                model_name,
                max_q_per_entry
            )

            current_entry_added_count = 0
            if generated_qas:
                with jsonlines.open(outfile, "a") as writer: # 'a'ãƒ¢ãƒ¼ãƒ‰ã§è¿½è¨˜
                    for qa_pair in generated_qas:
                        current_qa_tuple = (qa_pair.question, qa_pair.answer)
                        if current_qa_tuple not in existing_qa_set:
                            writer.write(qa_pair.model_dump())
                            existing_qa_set.add(current_qa_tuple)
                            total_newly_added_in_session += 1
                            current_entry_added_count += 1
                        else:
                            print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(é‡è¤‡): Q: {qa_pair.question}")
            
            if current_entry_added_count > 0:
                print(f"âœ¨ ã“ã®ã‚¨ãƒ³ãƒˆãƒªã§ {current_entry_added_count} ä»¶ã‚’æ–°ãŸã«æ›¸ãå‡ºã—ã¾ã—ãŸã€‚")
            else:
                print("â„¹ï¸ ã“ã®ã‚¨ãƒ³ãƒˆãƒªã§ã¯æ–°ã—ã„Q&Aã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            processed_entry_count += 1
            await asyncio.sleep(0.5) # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ç­‰ã‚’è€ƒæ…®ã—ãŸçŸ­ã„å¾…æ©Ÿ (WebSearch.pyã‚ˆã‚ŠçŸ­ãã¦ã‚‚è‰¯ã„ã‹ã‚‚)

    print(f"\\n--- å…¨å‡¦ç†å®Œäº† ---")
    print(f"ğŸ‰ åˆè¨ˆ {total_newly_added_in_session} ä»¶ã®æ–°ã—ã„Q&Aã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã«æ›¸ãå‡ºã—ã¾ã—ãŸ â†’ {outfile}")


if __name__ == "__main__":
    # --- WebSearch.pyã‹ã‚‰æµç”¨: argparseã®è¨­å®šã‚’æ”¹å¤‰ ---
    parser = argparse.ArgumentParser(description="JSONLãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã™ã€‚WebSearch.pyã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æµç”¨ã€‚")
    parser.add_argument(
        "--input_jsonl",
        type=str,
        required=True,
        help="å…¥åŠ›JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ (ä¾‹: /app/aflac_with_body.jsonl)"
    )
    parser.add_argument(
        "--outfile",
        type=str,
        default="generated_qa_from_jsonl_alt.jsonl",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: output_qa_alt.jsonl)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«å (ä¾‹: gpt-4o, gpt-4-turbo)"
    )
    parser.add_argument(
        "--source_id_field",
        type=str,
        required=True,
        help="JSONLã‚¨ãƒ³ãƒˆãƒªå†…ã§ã‚½ãƒ¼ã‚¹è­˜åˆ¥å­(URLç­‰)ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å (ä¾‹: url, id)"
    )
    parser.add_argument(
        "--content_field",
        type=str,
        required=True,
        help="JSONLã‚¨ãƒ³ãƒˆãƒªå†…ã§Q&Aç”Ÿæˆã®å…ƒã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å (ä¾‹: response_text, body)"
    )
    parser.add_argument(
        "--max_q_per_entry",
        type=int,
        default=2, # WebSearch.pyã®max_q_per_urlã«ç›¸å½“
        help="1ã¤ã®JSONLã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰ç”Ÿæˆã™ã‚‹Q&Aã®æœ€å¤§æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2)"
    )
    parser.add_argument(
        "--max_entries",
        type=int,
        default=-1, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨ä»¶å‡¦ç†
        help="å‡¦ç†ã™ã‚‹JSONLã‚¨ãƒ³ãƒˆãƒªã®æœ€å¤§æ•°ã€‚ãƒ†ã‚¹ãƒˆç”¨ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: -1 ã§å…¨ä»¶å‡¦ç†)"
    )
    # WebSearch.pyã«ã‚ã£ãŸ --urls_file, --max_tries_per_url, --max_urls ã¯ä¸è¦ãªã®ã§å‰Šé™¤

    args = parser.parse_args()

    asyncio.run(process_jsonl_and_generate_qa(
        args.input_jsonl,
        args.outfile,
        args.model,
        args.source_id_field,
        args.content_field,
        args.max_q_per_entry,
        args.max_entries
    ))

"""
ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¾‹:

python /app/Create_QA/src/Create_QA_from_jsonl_alt.py \
    --input_jsonl /app/aflac_tsumitasu_qa.jsonl \
    --outfile /app/Create_QA/output/QA/aflac_tsumitasu_qa_gen_alt.jsonl \
    --model gpt-4o-mini \
    --source_id_field url \
    --content_field answer \
    --max_q_per_entry 1 \
    --max_entries 3

èª¬æ˜:
--input_jsonl: å…¥åŠ›ã™ã‚‹JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚
--outfile: ç”Ÿæˆã•ã‚ŒãŸQ&Aã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚
--model: Q&Aç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹OpenAIã®ãƒ¢ãƒ‡ãƒ«åã€‚
--source_id_field: å…¥åŠ›JSONLå†…ã§ã€å„Q&Aã®å‡ºå…¸å…ƒURLãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã€‚
--content_field: å…¥åŠ›JSONLå†…ã§ã€Q&Aç”Ÿæˆã®å…ƒã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã€‚
                 (ã“ã®ä¾‹ã§ã¯æ—¢å­˜ã®å›ç­” 'answer' ã‚’å…ƒã«æ–°ã—ã„Q&Aã‚’ç”Ÿæˆã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ãŒã€
                  é€šå¸¸ã¯ã‚ˆã‚Šåºƒç¯„ãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ä¾‹: 'body', 'response_text')
--max_q_per_entry: 1ã¤ã®å…¥åŠ›ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰ç”Ÿæˆã™ã‚‹Q&Aã®æœ€å¤§æ•°ã€‚
--max_entries: å‡¦ç†ã™ã‚‹å…¥åŠ›ã‚¨ãƒ³ãƒˆãƒªã®æœ€å¤§æ•° (ãƒ†ã‚¹ãƒˆç”¨ãªã©ã«ä½¿ç”¨)ã€‚-1ã§å…¨ä»¶å‡¦ç†ã€‚
"""
