\
import asyncio
import jsonlines
import os
import argparse
from typing import List, Set, Tuple, Dict, Any, Optional
from pydantic import BaseModel
from dotenv import load_dotenv

# agentsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒ Create_QA ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã¨ä»®å®š
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from agents import Agent, Runner # agentsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰Agentã¨Runnerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

load_dotenv("/app/.env", override=True)

# --- WebSearch.pyã‹ã‚‰æµç”¨ ---
class QAPair(BaseModel):
    question: str
    answer: str
    source_url: str # JSONLã®å„ã‚¨ãƒ³ãƒˆãƒªã®å‡ºå…¸ã‚’ç¤ºã™ãŸã‚ã«æµç”¨
    questioner_persona: str # è¿½åŠ : ã©ã®ã‚ˆã†ãªäººãŒã™ã‚‹è³ªå•ã‹
    information_category: str  # è¿½åŠ : æƒ…å ±ã®ã‚«ãƒ†ã‚´ãƒª
    related_keywords: List[str] # è¿½åŠ : é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

# --- WebSearch.pyã®generate_qa_for_urlã‚’æ”¹å¤‰: å˜ä¸€Q&Aç”Ÿæˆæ–¹å¼ ---
async def generate_single_qa(
    source_identifier: str, # URLã‚„ãƒ•ã‚¡ã‚¤ãƒ«åãªã©ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡ºå…¸
    text_content: str,
    existing_qa_for_source_display: List[str],
    model_name: str,
    attempt_number: int  # ä½•å›ç›®ã®è©¦è¡Œã‹ã‚’æ˜ç¤º
) -> Optional[QAPair]:
    """
    1ã¤ã®Q&Aãƒšã‚¢ã®ã¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    if not existing_qa_for_source_display:
        existing_qa_instructions_segment = "There are currently no existing Q&A pairs for this source."
    else:
        existing_qa_str = "\\\\n".join(existing_qa_for_source_display)
        existing_qa_instructions_segment = (
            f"The following Q&A pairs already exist for this source ({source_identifier}):\\\\n"
            f"Please generate a NEW Q&A pair that covers different aspects or provides different perspectives.\\\\n"
            f"---Existing Q&A Start---\\\\n"
            f"{existing_qa_str}\\\\n"
            f"---Existing Q&A End---"
        )

    qa_agent = Agent(
        name="Single QA Generator",
        instructions=(
            "You are a specialized knowledge extraction assistant.\\\\n"
            f"1. Analyze the provided text content from: {source_identifier} (likely a life insurance company's webpage).\\\\n"
            f"2. Text content: \\\\\\\\n---TEXT CONTENT BEGIN---\\\\\\\\n{text_content}\\\\\\\\n---TEXT CONTENT END---\\\\\\\\n"
            f"3. {existing_qa_instructions_segment}\\\\\\\\n"
            f"4. Generate ONE high-quality question-answer pair that would be valuable for an FAQ. Focus on:\\\\n"
            "   - Creating a natural, specific question someone would actually ask\\\\n"
            "   - If the answer varies based on conditions (age, gender, health status, contract details, timing, etc.), make the question specify those conditions clearly\\\\n"
            "   - If the answer differs by insurance product, include the specific product name in the question\\\\n"
            "   - For example, instead of 'ä¿é™ºé‡‘ã¯ã„ãã‚‰ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ' ask '30æ­³ç”·æ€§ãŒã¡ã‚ƒã‚“ã¨å¿œãˆã‚‹åŒ»ç™‚ä¿é™ºEVERã«åŠ å…¥ã—ãŸå ´åˆã€å…¥é™¢çµ¦ä»˜é‡‘ã¯ã„ãã‚‰ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ'\\\\n"
            "   - Another example: instead of 'ä¿é™ºæ–™ã®æ”¯æ‰•ã„æ–¹æ³•ã¯ï¼Ÿ' ask 'ã‚¢ãƒ•ãƒ©ãƒƒã‚¯ã®ãŒã‚“ä¿é™ºãƒ•ã‚©ãƒ«ãƒ†ã®ä¿é™ºæ–™æ”¯æ‰•ã„æ–¹æ³•ã«ã¯ã©ã®ã‚ˆã†ãªé¸æŠè‚¢ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ'\\\\n"
            "   - Providing a comprehensive, self-contained answer that addresses the specific conditions and products mentioned in the question\\\\n"
            "   - Avoiding generic or overly broad questions that could have multiple different answers\\\\n"
            "   - Including relevant details and context\\\\n"
            f"5. This is attempt #{attempt_number}, so try to find a unique angle or aspect not covered before.\\\\n"
            "6. The question, answer, questioner_persona, information_category, and related_keywords MUST be in Japanese.\\\\n"
            "7. The answer should be self-contained and directly address the question. Avoid answers that primarily redirect the user elsewhere.\\\\n"
            "8. Each Q&A must include:\\\\n"
            "   a. The question (in Japanese)\\\\n"
            "   b. The answer (in Japanese)\\\\n"
            f"   c. The source identifier: '{source_identifier}'\\\\n"
            "   d. A questioner_persona appropriate for a life insurance website visitor (e.g., 'å¥‘ç´„æ¤œè¨ä¸­ã®é¡§å®¢', 'æ—¢å¥‘ç´„è€…', 'ä¿é™ºé‡‘å—å–äºº', 'å°±è·æ´»å‹•ä¸­ã®å­¦ç”Ÿ', 'ä¸€èˆ¬çš„ãªæƒ…å ±åé›†è€…')\\\\n"
            "   e. An information_category (e.g., 'å¥‘ç´„æ‰‹ç¶šã', 'ä¿éšœå†…å®¹', 'ä¿é™ºé‡‘è«‹æ±‚', 'å•†å“æ¯”è¼ƒ', 'ç¨é‡‘ãƒ»æ§é™¤', 'å¥åº·å¢—é€²ã‚µãƒ¼ãƒ“ã‚¹', 'ä¼šç¤¾æƒ…å ±')\\\\n"
            "   f. A list of 3-5 related_keywords\\\\n"
            "Return exactly ONE QAPair object with all required fields."
        ),
        output_type=QAPair,  # å˜ä¸€ã®QAPairã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        model=model_name,
    )

    result = await Runner.run(qa_agent, input=f"Generate one high-quality Q&A for content from {source_identifier}")
    
    if result.final_output:
        qa = result.final_output
        # source_urlã®ä¿®æ­£
        if qa.source_url != source_identifier:
            qa_dict = qa.model_dump()
            qa_dict["source_url"] = source_identifier
            return QAPair(**qa_dict)
        return qa
    return None

# --- ä¸¦åˆ—å‡¦ç†å¯¾å¿œ: ãƒ•ã‚¡ã‚¤ãƒ«I/O ãƒ­ãƒƒã‚¯ç®¡ç† ---
import threading
import time
from datetime import datetime

# ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ç”¨ãƒ­ãƒƒã‚¯
file_lock = threading.Lock()

def collect_existing_qa_for_source(source_identifier: str, outfile: str) -> List[str]:
    """
    æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹IDã«é–¢ã™ã‚‹æ—¢å­˜Q&Aã‚’åé›†
    """
    existing_qa_display = []
    if os.path.exists(outfile):
        try:
            with file_lock:  # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ™‚ã‚‚ãƒ­ãƒƒã‚¯
                with jsonlines.open(outfile, "r") as reader:
                    for qa_obj_dict in reader:
                        if qa_obj_dict.get("source_url") == source_identifier:
                            q = qa_obj_dict.get("question")
                            a = qa_obj_dict.get("answer")
                            if q and a:
                                existing_qa_display.append(f"- Q: {q}\\n  A: {a}")
        except Exception as e:
            print(f"è­¦å‘Š: æ—¢å­˜Q&Aåé›†ä¸­ã«ã‚¨ãƒ©ãƒ¼ ({source_identifier}): {e}")
    return existing_qa_display

def save_qa_to_file(qa: QAPair, outfile: str) -> bool:
    """
    Q&Aã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å®‰å…¨ã«ä¿å­˜
    """
    try:
        with file_lock:  # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿æ™‚ã®ãƒ­ãƒƒã‚¯
            with jsonlines.open(outfile, "a") as writer:
                writer.write(qa.model_dump())
        return True
    except Exception as e:
        print(f"Q&Aä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def process_single_entry(
    entry_data: Tuple[int, Dict[str, Any]],
    outfile: str,
    model_name: str,
    source_id_field: str,
    content_field: str,
    max_q_per_entry: int,
    global_existing_qa_set: Set[Tuple[str, str]]
) -> int:
    """
    å˜ä¸€ã‚¨ãƒ³ãƒˆãƒªã®å‡¦ç†ï¼ˆã‚¨ãƒ³ãƒˆãƒªå†…ã®Q&Aç”Ÿæˆã¯é€æ¬¡å®Ÿè¡Œï¼‰
    """
    i, entry = entry_data
    
    source_identifier = entry.get(source_id_field)
    text_content = entry.get(content_field)
    
    if not source_identifier:
        print(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: '{source_id_field}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return 0
    if not text_content:
        print(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: '{content_field}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return 0

    print(f"ğŸ”„ ã‚¨ãƒ³ãƒˆãƒª {i+1} ã‚’å‡¦ç†ä¸­: {source_identifier}")

    # ã“ã®ã‚½ãƒ¼ã‚¹ã®æ—¢å­˜Q&Aã‚’åé›†
    existing_qa_for_current_source_display = collect_existing_qa_for_source(source_identifier, outfile)
    
    # ã‚¨ãƒ³ãƒˆãƒªå†…ã§ã®Q&Aç”Ÿæˆã¯é€æ¬¡å®Ÿè¡Œï¼ˆå“è³ªé‡è¦–ï¼‰
    current_entry_added_count = 0
    for attempt in range(max_q_per_entry):
        print(f"  ğŸ“ ã‚¨ãƒ³ãƒˆãƒª {i+1}, è©¦è¡Œ {attempt + 1}/{max_q_per_entry}")
        
        single_qa = await generate_single_qa(
            source_identifier,
            text_content,
            existing_qa_for_current_source_display,
            model_name,
            attempt + 1
        )
        
        if single_qa:
            current_qa_tuple = (single_qa.question, single_qa.answer)
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
            with file_lock:
                is_duplicate = current_qa_tuple in global_existing_qa_set
                if not is_duplicate:
                    global_existing_qa_set.add(current_qa_tuple)
            
            if not is_duplicate:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                if save_qa_to_file(single_qa, outfile):
                    # æ¬¡ã®è©¦è¡Œã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ã“ã®ã‚¨ãƒ³ãƒˆãƒªã®æ—¢å­˜ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    existing_qa_for_current_source_display.append(
                        f"- Q: {single_qa.question}\\n  A: {single_qa.answer}"
                    )
                    current_entry_added_count += 1
                    print(f"    âœ… Q&Aç”ŸæˆæˆåŠŸ: {single_qa.question[:50]}...")
                else:
                    print(f"    âŒ Q&Aä¿å­˜å¤±æ•—")
            else:
                print(f"    âš ï¸ é‡è¤‡ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {single_qa.question[:50]}...")
        else:
            print(f"    âŒ Q&Aç”Ÿæˆå¤±æ•—")
        
        # APIåˆ¶é™å¯¾å¿œã®å¾…æ©Ÿ
        await asyncio.sleep(1)
    
    if current_entry_added_count > 0:
        print(f"âœ¨ ã‚¨ãƒ³ãƒˆãƒª {i+1}: {current_entry_added_count} ä»¶ã‚’æ–°ãŸã«ç”Ÿæˆ")
    else:
        print(f"â„¹ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: æ–°ã—ã„Q&Aã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    # ã‚¨ãƒ³ãƒˆãƒªé–“ã®å¾…æ©Ÿ
    await asyncio.sleep(0.5)
    return current_entry_added_count

# --- ã‚¨ãƒ³ãƒˆãƒªãƒ¬ãƒ™ãƒ«ä¸¦åˆ—å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³é–¢æ•° ---
async def process_jsonl_parallel_entries(
    input_jsonl_path: str,
    outfile: str,
    model_name: str,
    source_id_field: str,
    content_field: str,
    max_q_per_entry: int = 3,
    max_entries_to_process: int = -1,
    max_concurrent_entries: int = 3  # åŒæ™‚å‡¦ç†ã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªæ•°
):
    """
    ã‚¨ãƒ³ãƒˆãƒªãƒ¬ãƒ™ãƒ«ä¸¦åˆ—å‡¦ç†ã§JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    """
    if not os.path.exists(input_jsonl_path):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{input_jsonl_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æ—¢å­˜Q&Aã®èª­ã¿è¾¼ã¿
    global_existing_qa_set: Set[Tuple[str, str]] = set()
    if os.path.exists(outfile):
        try:
            with jsonlines.open(outfile, "r") as reader:
                for qa_obj_dict in reader:
                    global_existing_qa_set.add((qa_obj_dict.get("question"), qa_obj_dict.get("answer")))
            print(f"ğŸ“‚ æ—¢å­˜ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{outfile}' ã‹ã‚‰ {len(global_existing_qa_set)} ä»¶ã®Q&Aã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: æ—¢å­˜ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{outfile}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # ã‚¨ãƒ³ãƒˆãƒªã‚’èª­ã¿è¾¼ã¿
    entries = []
    with jsonlines.open(input_jsonl_path, "r") as reader:
        for i, entry in enumerate(reader):
            if max_entries_to_process != -1 and i >= max_entries_to_process:
                break
            entries.append((i, entry))

    print(f"ğŸš€ {len(entries)} ã‚¨ãƒ³ãƒˆãƒªã‚’æœ€å¤§ {max_concurrent_entries} ä¸¦åˆ—ã§å‡¦ç†é–‹å§‹")
    print(f"âš™ï¸ è¨­å®š: ãƒ¢ãƒ‡ãƒ«={model_name}, ã‚¨ãƒ³ãƒˆãƒªã‚ãŸã‚ŠQ&Aæ•°={max_q_per_entry}")
    
    start_time = time.time()

    # ä¸¦åˆ—å‡¦ç†ç”¨ã‚»ãƒãƒ•ã‚©
    semaphore = asyncio.Semaphore(max_concurrent_entries)
    
    async def process_entry_with_semaphore(entry_data):
        async with semaphore:
            return await process_single_entry(
                entry_data,
                outfile,
                model_name,
                source_id_field,
                content_field,
                max_q_per_entry,
                global_existing_qa_set
            )
    
    # ä¸¦åˆ—å®Ÿè¡Œ
    tasks = [process_entry_with_semaphore(entry_data) for entry_data in entries]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # çµæœé›†è¨ˆ
    total_newly_added = sum(r for r in results if isinstance(r, int))
    error_count = sum(1 for r in results if isinstance(r, Exception))
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"\nğŸ“Š å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼")
    print(f"=" * 50)
    print(f"ğŸ‰ æ–°è¦Q&Aç”Ÿæˆæ•°: {total_newly_added} ä»¶")
    print(f"â±ï¸ å‡¦ç†æ™‚é–“: {processing_time:.2f} ç§’")
    print(f"âš¡ å¹³å‡å‡¦ç†é€Ÿåº¦: {len(entries) / processing_time:.2f} ã‚¨ãƒ³ãƒˆãƒª/ç§’")
    if error_count > 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã‚¨ãƒ³ãƒˆãƒªæ•°: {error_count} ä»¶")
    print(f"ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {outfile}")

# --- ä¸‹ä½äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ ---
async def process_jsonl_single_qa_mode(
    input_jsonl_path: str,
    outfile: str,
    model_name: str,
    source_id_field: str,
    content_field: str,
    max_q_per_entry: int = 3,
    max_entries_to_process: int = -1,
):
    """
    ä¸‹ä½äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°ï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆã‚’å‘¼ã³å‡ºã—ï¼‰
    """
    await process_jsonl_parallel_entries(
        input_jsonl_path,
        outfile,
        model_name,
        source_id_field,
        content_field,
        max_q_per_entry,
        max_entries_to_process,
        max_concurrent_entries=1  # é€æ¬¡å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
    )


if __name__ == "__main__":
    # --- WebSearch.pyã‹ã‚‰æµç”¨: argparseã®è¨­å®šã‚’æ”¹å¤‰ ---
    parser = argparse.ArgumentParser(description="JSONLãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã™ã€‚WebSearch.pyã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æµç”¨ã€‚")
    parser.add_argument(
        "--input_jsonl",
        type=str,
        required=True,
        help="å…¥åŠ›JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ (ä¾‹: /app/aflac_with_body.jsonl)"
    )
    parser.add_argument(
        "--outfile",
        type=str,
        default="generated_qa_from_jsonl_alt.jsonl",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: output_qa_alt.jsonl)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«å (ä¾‹: gpt-4o, gpt-4-turbo)"
    )
    parser.add_argument(
        "--source_id_field",
        type=str,
        required=True,
        help="JSONLã‚¨ãƒ³ãƒˆãƒªå†…ã§ã‚½ãƒ¼ã‚¹è­˜åˆ¥å­(URLç­‰)ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å (ä¾‹: url, id)"
    )
    parser.add_argument(
        "--content_field",
        type=str,
        required=True,
        help="JSONLã‚¨ãƒ³ãƒˆãƒªå†…ã§Q&Aç”Ÿæˆã®å…ƒã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å (ä¾‹: response_text, body)"
    )
    parser.add_argument(
        "--max_q_per_entry",
        type=int,
        default=2, # WebSearch.pyã®max_q_per_urlã«ç›¸å½“
        help="1ã¤ã®JSONLã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰ç”Ÿæˆã™ã‚‹Q&Aã®æœ€å¤§æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2)"
    )
    parser.add_argument(
        "--max_entries",
        type=int,
        default=-1, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨ä»¶å‡¦ç†
        help="å‡¦ç†ã™ã‚‹JSONLã‚¨ãƒ³ãƒˆãƒªã®æœ€å¤§æ•°ã€‚ãƒ†ã‚¹ãƒˆç”¨ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: -1 ã§å…¨ä»¶å‡¦ç†)"
    )
    parser.add_argument(
        "--max_concurrent",
        type=int,
        default=3,
        help="åŒæ™‚å‡¦ç†ã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3, 1ã§é€æ¬¡å‡¦ç†)"
    )
    # WebSearch.pyã«ã‚ã£ãŸ --urls_file, --max_tries_per_url, --max_urls ã¯ä¸è¦ãªã®ã§å‰Šé™¤

    args = parser.parse_args()

    asyncio.run(process_jsonl_parallel_entries(
        args.input_jsonl,
        args.outfile,
        args.model,
        args.source_id_field,
        args.content_field,
        args.max_q_per_entry,
        args.max_entries,
        args.max_concurrent  # ä¸¦åˆ—æ•°ã®æŒ‡å®š
    ))

"""
ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¾‹:

# åŸºæœ¬å®Ÿè¡Œï¼ˆ3ã‚¨ãƒ³ãƒˆãƒªä¸¦åˆ—å‡¦ç†ï¼‰
python /app/Create_QA/src/Create_QA_from_jsonl_alt.py \
    --input_jsonl /app/aflac_with_body.jsonl \
    --outfile /app/Create_QA/output/QA/aflac_qa_parallel.jsonl \
    --model gpt-4o-mini \
    --source_id_field url \
    --content_field response_text \
    --max_q_per_entry 2 \
    --max_concurrent 3

# é«˜é€Ÿå‡¦ç†ï¼ˆ5ã‚¨ãƒ³ãƒˆãƒªä¸¦åˆ—ï¼‰
python /app/Create_QA/src/Create_QA_from_jsonl_alt.py \
    --input_jsonl /app/aflac_with_body.jsonl \
    --outfile /app/Create_QA/output/QA/aflac_qa_fast.jsonl \
    --model gpt-4o-mini \
    --source_id_field url \
    --content_field response_text \
    --max_q_per_entry 3 \
    --max_concurrent 5 \
    --max_entries 20

# é€æ¬¡å‡¦ç†ï¼ˆå¾“æ¥ãƒ¢ãƒ¼ãƒ‰ï¼‰
python /app/Create_QA/src/Create_QA_from_jsonl_alt.py \
    --input_jsonl /app/aflac_tsumitasu_qa.jsonl \
    --outfile /app/Create_QA/output/QA/aflac_tsumitasu_qa_sequential.jsonl \
    --model gpt-4o-mini \
    --source_id_field url \
    --content_field answer \
    --max_q_per_entry 1 \
    --max_concurrent 1 \
    --max_entries 3

ã€æ”¹è‰¯ç‰ˆã€‘ã‚¨ãƒ³ãƒˆãƒªãƒ¬ãƒ™ãƒ«ä¸¦åˆ—å‡¦ç†å¯¾å¿œ:
- ç•°ãªã‚‹source_identifierã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä¸¦åˆ—ã§å‡¦ç†
- å„ã‚¨ãƒ³ãƒˆãƒªå†…ã®Q&Aç”Ÿæˆã¯é€æ¬¡å®Ÿè¡Œã§å“è³ªã‚’ä¿æŒ
- ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªãƒ•ã‚¡ã‚¤ãƒ«I/Oå‡¦ç†
- é‡è¤‡æ’é™¤ã®ä¸¦åˆ—å¯¾å¿œ
- è©³ç´°ãªé€²æ—è¡¨ç¤ºã¨å‡¦ç†çµ±è¨ˆ
- 3-5å€ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾

èª¬æ˜:
--input_jsonl: å…¥åŠ›ã™ã‚‹JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚
--outfile: ç”Ÿæˆã•ã‚ŒãŸQ&Aã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚
--model: Q&Aç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹OpenAIã®ãƒ¢ãƒ‡ãƒ«åã€‚
--source_id_field: å…¥åŠ›JSONLå†…ã§ã€å„Q&Aã®å‡ºå…¸å…ƒURLãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã€‚
--content_field: å…¥åŠ›JSONLå†…ã§ã€Q&Aç”Ÿæˆã®å…ƒã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã€‚
--max_q_per_entry: 1ã¤ã®å…¥åŠ›ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰ç”Ÿæˆã™ã‚‹Q&Aã®æœ€å¤§æ•°ã€‚
--max_entries: å‡¦ç†ã™ã‚‹å…¥åŠ›ã‚¨ãƒ³ãƒˆãƒªã®æœ€å¤§æ•° (ãƒ†ã‚¹ãƒˆç”¨ãªã©ã«ä½¿ç”¨)ã€‚-1ã§å…¨ä»¶å‡¦ç†ã€‚
--max_concurrent: åŒæ™‚å‡¦ç†ã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªæ•°ã€‚1ã§é€æ¬¡å‡¦ç†ã€3-5ã§ä¸¦åˆ—å‡¦ç†æ¨å¥¨ã€‚
"""
